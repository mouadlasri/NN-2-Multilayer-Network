{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4e41fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline  \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import colorsys\n",
    "import sys\n",
    "import time\n",
    "from IPython import display\n",
    "from mpl_toolkits import mplot3d\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49855cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(X):\n",
    "    return np.maximum(X,0)\n",
    "\n",
    "def relu_derivative(X):\n",
    "    return 1.0 * (X>0) #.astype(float)\n",
    "\n",
    "def tanh(X):\n",
    "    return np.tanh(X)\n",
    "\n",
    "def tanh_derivative(X):\n",
    "    return (1.0-tanh(X)**2)\n",
    "\n",
    "def logistic(X):\n",
    "    return 1.0/(1.0+np.exp(-X))\n",
    "\n",
    "def logistic_derivative(X):\n",
    "    return (logistic(X)*(1.0-logistic(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4a92e92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a two-layer neural network\n",
    "def create_model(X,hidden_nodes,output_dim=2,activation_function='relu'):\n",
    "    # this will hold a dictionary of layers\n",
    "    model = {}\n",
    "    \n",
    "    # set the model activation function, the eval() function converts a string into a function\n",
    "    # this way, we can directly call the appropriate activation function and its derative with just the string name\n",
    "    # and we can avoid writing \"if\" statements for each activation function and derivatives\n",
    "    model['activation_function'] = eval(activation_function);\n",
    "    \n",
    "    # set the model activation function derative using eval(), same logic as previous line\n",
    "    model['activation_function_derivative'] = eval(activation_function + '_derivative')\n",
    "    \n",
    "    # input dimensionality\n",
    "    input_dim = X.shape[1]\n",
    "    \n",
    "    # first set of weights from input to hidden layer 1\n",
    "    model['W1'] = np.random.randn(input_dim, hidden_nodes)/np.sqrt(input_dim)\n",
    "    # set of biases\n",
    "    model['b1'] = np.zeros((1, hidden_nodes))\n",
    "    \n",
    "    # second set of weights from hidden layer 1 to layer 2\n",
    "    model['W2'] = np.random.randn(hidden_nodes, output_dim)/np.sqrt(hidden_nodes)\n",
    "    # set of biases for second hidden layer\n",
    "    model['b2'] = np.zeroes((1, hidden_nodes))\n",
    "    \n",
    "    # third set of weights from hidder layer 2 to output\n",
    "    model['W2'] = np.random.randn(hidden_nodes, output_dim)/np.sqrt(hidden_nodes)\n",
    "    # set of biases for output layer\n",
    "    model['b2'] = np.zeros((1, output_dim))\n",
    "    \n",
    "    return model\n",
    "\n",
    "# defines the forward pass given a model and data\n",
    "def feed_forward(model, x):\n",
    "    # get weights and biases\n",
    "    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n",
    "    # first layer\n",
    "    z1 = x.dot(W1) + b1\n",
    "    \n",
    "    # activation function\n",
    "    #a1 = logistic(z1)\n",
    "    #a1 = tanh(z1)\n",
    "    a1 = model['activation_function'](z1)\n",
    "    \n",
    "    # second layer\n",
    "    z2 = a1.dot(W2)+b2\n",
    "    \n",
    "    # no activation function as this is simply a linear layer!!\n",
    "    out = z2\n",
    "    return z1, a1, z2, out\n",
    "\n",
    "# define the regression loss\n",
    "def calculate_loss(model,X,y,reg_lambda):\n",
    "    num_examples = X.shape[0]\n",
    "    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n",
    "    \n",
    "    # what are the current predictions\n",
    "    z1, a1, z2, out = feed_forward(model, X)\n",
    "    \n",
    "    # calculate L2 loss\n",
    "    loss = 0.5*np.sum((out-y)**2)\n",
    "    \n",
    "    # add regulatization term to loss\n",
    "    loss += reg_lambda/2*(np.sum(np.square(W1))+ np.sum(np.square(W2)))\n",
    "    \n",
    "    # return per-item loss\n",
    "    return 1./num_examples * loss\n",
    "\n",
    "# back-propagation for the two-layer network\n",
    "def backprop(X,y,model,z1,a1,z2,output,reg_lambda):\n",
    "    \n",
    "    # derivative of loss function\n",
    "    delta3 = (output-y)/X.shape[0]\n",
    "    # multiply this by activation outputs of hidden layer\n",
    "    dW2 = (a1.T).dot(delta3)\n",
    "    # and over all neurons\n",
    "    db2 = np.sum(delta3, axis =0, keepdims=True) #different because it is not being multiplied by all the weights, only 1\n",
    "    \n",
    "    # derivative of activation function\n",
    "    delta2 = delta3.dot(model['W2'].T)* model['activation_function_derivative'](a1) # call the appropriate activation function derivative\n",
    "    \n",
    "    # multiply by input data\n",
    "    dW1 = (X.T).dot(delta2)\n",
    "    # and sum over all neurons\n",
    "    db1 = np.sum(delta2, axis=0)\n",
    "    \n",
    "    # add regularization terms on the two weights\n",
    "    dW2 += reg_lambda * model['W2']\n",
    "    dW1 += reg_lambda * model['W1']\n",
    "    \n",
    "    return dW1, dW2, db1, db2\n",
    "\n",
    "# simple training loop\n",
    "def train(model, X, y, num_passes=100000, reg_lambda = 0.1, learning_rate = 0.001):\n",
    "    # whether to do stochastic gradient descent\n",
    "    sgd = True\n",
    "    \n",
    "    # variable that checks whether we break iteration\n",
    "    done = False\n",
    "    \n",
    "    # keeping track of losses\n",
    "    previous_loss = float('inf')\n",
    "    losses = []\n",
    "\n",
    "    # iteration counter\n",
    "    i = 0\n",
    "    while done == False:\n",
    "        if sgd:\n",
    "            # choose a random set of points\n",
    "            randinds = np.random.choice(np.arange(len(y)),30,False) #bad programming because we dont know the number of  data points needed , 30 is random\n",
    "            # get predictions\n",
    "            z1,a1,z2,output = feed_forward(model, X[randinds,:])\n",
    "            # feed this into backprop\n",
    "            dW1, dW2, db1, db2 = backprop(X[randinds,:],y[randinds],model,z1,a1,z2,output,reg_lambda)\n",
    "        else:\n",
    "            # get predictions\n",
    "            z1,a1,z2,output = feed_forward(model, X)\n",
    "            # feed this into backprop\n",
    "            dW1, dW2, db1, db2 = backprop(X,y,model,z1,a1,z2,output,reg_lambda)\n",
    "            \n",
    "        # given the results of backprop, update both weights and biases\n",
    "        model['W1'] -= learning_rate * dW1\n",
    "        model['b1'] -= learning_rate * db1\n",
    "        model['W2'] -= learning_rate * dW2\n",
    "        model['b2'] -= learning_rate * db2\n",
    "        \n",
    "        # do some book-keeping every once in a while\n",
    "        if i % 1000 == 0:\n",
    "            loss = calculate_loss(model, X, y, reg_lambda)\n",
    "            losses.append(loss)\n",
    "            print(\"Loss after iteration {}: {}\".format(i, loss))\n",
    "            # very crude method to break optimization\n",
    "            if np.abs((previous_loss-loss)/previous_loss) < 0.001:\n",
    "                done = True\n",
    "            previous_loss = loss\n",
    "        i += 1\n",
    "        if i>=num_passes:\n",
    "            done = True\n",
    "    return model, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e886fd20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_function': <function relu at 0x0000020C87689480>, 'activation_function_derivative': <function relu_derivative at 0x0000020CD7505E10>, 'W1': array([[-0.4699478 ,  0.97385053,  0.78625119, -0.52552253, -0.05038252,\n",
      "         0.28625191, -0.36754976, -0.07414273,  0.80879682, -0.07854602],\n",
      "       [ 0.3190304 , -1.64384229, -0.68509709,  0.55594788,  0.01703111,\n",
      "        -1.04160609, -0.05883318, -0.72216067,  0.3902574 , -0.20642454]]), 'b1': array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), 'W2': array([[ 0.30782197],\n",
      "       [-0.01210331],\n",
      "       [-0.341612  ],\n",
      "       [ 0.23950397],\n",
      "       [ 0.38028241],\n",
      "       [ 0.20555401],\n",
      "       [-0.00240822],\n",
      "       [ 0.07529288],\n",
      "       [ 0.01969584],\n",
      "       [ 0.11260976]]), 'b2': array([[0.]])}\n",
      "Loss after iteration 0: 1498.6878800906895\n",
      "Loss after iteration 1000: 25.30862710487808\n",
      "Loss after iteration 2000: 9.79275786461649\n",
      "Loss after iteration 3000: 7.173275312638423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mouad\\AppData\\Local\\Temp\\ipykernel_22820\\829421872.py:127: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if np.abs((previous_loss-loss)/previous_loss) < 0.001:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 4000: 5.673610204388251\n",
      "Loss after iteration 5000: 5.207344884467192\n",
      "Loss after iteration 6000: 4.862537249340139\n",
      "Loss after iteration 7000: 4.0757602101152814\n",
      "Loss after iteration 8000: 3.8069720236146876\n",
      "Loss after iteration 9000: 3.2283361008171716\n",
      "Loss after iteration 10000: 3.1573898333662034\n",
      "Loss after iteration 11000: 3.108284907481881\n",
      "Loss after iteration 12000: 3.0550558558941234\n",
      "Loss after iteration 13000: 3.1170783268725377\n",
      "Loss after iteration 14000: 2.9492997196211497\n",
      "Loss after iteration 15000: 2.991143840487557\n",
      "Loss after iteration 16000: 3.0207746040675705\n",
      "Loss after iteration 17000: 2.861867627714464\n",
      "Loss after iteration 18000: 3.041911281662467\n",
      "Loss after iteration 19000: 2.98107900618627\n",
      "Loss after iteration 20000: 2.9616576115393864\n",
      "Loss after iteration 21000: 2.9345325276155285\n",
      "Loss after iteration 22000: 2.843377682660933\n",
      "Loss after iteration 23000: 2.852296427647476\n",
      "Loss after iteration 24000: 2.829184118621967\n",
      "Loss after iteration 25000: 3.1258934821372284\n",
      "Loss after iteration 26000: 2.827837325310373\n",
      "Loss after iteration 27000: 2.8142020448562146\n",
      "Loss after iteration 28000: 2.7817867987837115\n",
      "Loss after iteration 29000: 2.886575135366549\n",
      "Loss after iteration 30000: 2.869099541132605\n",
      "Loss after iteration 31000: 2.7799870269968627\n",
      "Loss after iteration 32000: 2.7699455100424752\n",
      "Loss after iteration 33000: 2.951231163013103\n",
      "Loss after iteration 34000: 2.7930735206329227\n",
      "Loss after iteration 35000: 2.8025930015849583\n",
      "Loss after iteration 36000: 2.76722425475404\n",
      "Loss after iteration 37000: 3.0538990542832805\n",
      "Loss after iteration 38000: 2.743366702696449\n",
      "Loss after iteration 39000: 2.7311398341577755\n",
      "Loss after iteration 40000: 2.8713982774362936\n",
      "Loss after iteration 41000: 2.7714989001670944\n",
      "Loss after iteration 42000: 2.7773457545286697\n",
      "Loss after iteration 43000: 2.7223047106533653\n",
      "Loss after iteration 44000: 2.8001177425021004\n",
      "Loss after iteration 45000: 2.8053237912782536\n",
      "Loss after iteration 46000: 2.75075719713839\n",
      "Loss after iteration 47000: 2.733679465969844\n",
      "Loss after iteration 48000: 2.8164032326508543\n",
      "Loss after iteration 49000: 2.7632867550118956\n",
      "Loss after iteration 50000: 2.7939035781200703\n",
      "Loss after iteration 51000: 2.721191799502264\n",
      "Loss after iteration 52000: 2.7934423383065066\n",
      "Loss after iteration 53000: 2.764200853726942\n",
      "Loss after iteration 54000: 2.7065783886294468\n",
      "Loss after iteration 55000: 2.705597968886066\n"
     ]
    }
   ],
   "source": [
    "numDataOne = 15\n",
    "numData = numDataOne*numDataOne\n",
    "# create data for regression\n",
    "xs=np.linspace(-8,8,numDataOne)\n",
    "ys=np.linspace(-8,8,numDataOne)\n",
    "counter=0\n",
    "X=np.zeros((numData,2))\n",
    "y=np.zeros((numData,1))\n",
    "for r in np.arange(0,numDataOne):\n",
    "    for c in np.arange(0,numDataOne):\n",
    "        X[counter,:]=[xs[r],ys[c]]\n",
    "        y[counter]=xs[r]**2+ys[c]**2+1\n",
    "        counter=counter+1\n",
    "\n",
    "# training set size\n",
    "num_examples = len(X) \n",
    "# input layer dimensionality\n",
    "nn_input_dim = 2 \n",
    "# output layer dimensionality\n",
    "nn_output_dim = 1  \n",
    "# learning rate for gradient descent\n",
    "learning_rate = 0.001\n",
    "# regularization strength\n",
    "reg_lambda = 0.01 \n",
    "\n",
    "# create the model\n",
    "model = create_model(X,10,1) # try changing number of neurons from 10 to 4 to 2 to 3 to 100 (to see how the final 3d image is altered)\n",
    "print(model)\n",
    "# train it\n",
    "model, losses = train(model,X, y, reg_lambda=reg_lambda, learning_rate=learning_rate)\n",
    "\n",
    "# determine predictions of the trained model\n",
    "output = feed_forward(model, X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
