{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4e41fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline  \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import colorsys\n",
    "import sys\n",
    "import time\n",
    "from IPython import display\n",
    "from mpl_toolkits import mplot3d\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49855cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(X):\n",
    "    return np.maximum(X,0)\n",
    "\n",
    "def relu_derivative(X):\n",
    "    return 1.0 * (X>0) #.astype(float)\n",
    "\n",
    "def tanh(X):\n",
    "    return np.tanh(X)\n",
    "\n",
    "def tanh_derivative(X):\n",
    "    return (1.0-tanh(X)**2)\n",
    "\n",
    "def logistic(X):\n",
    "    return 1.0/(1.0+np.exp(-X))\n",
    "\n",
    "def logistic_derivative(X):\n",
    "    return (logistic(X)*(1.0-logistic(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4a92e92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a two-layer neural network\n",
    "def create_model(X,hidden_nodes,output_dim=2,activation_function='relu'):\n",
    "    \n",
    "    # this will hold a dictionary of layers and which activation_function to call\n",
    "    model = {}\n",
    "    \n",
    "    # save which activation function in the moidel.\n",
    "    # the eval() function converts a string into a function\n",
    "    # this way, we can directly call the appropriate activation function and its derative with just the string name\n",
    "    # and we can avoid writing \"if\" statements for each activation function and derivatives\n",
    "    model['activation_function'] = eval(activation_function);\n",
    "    \n",
    "    # set the model activation function derative using eval(), same logic as previous line\n",
    "    model['activation_function_derivative'] = eval(activation_function + '_derivative')\n",
    "    \n",
    "    # input dimensionality\n",
    "    input_dim = X.shape[1]\n",
    "    \n",
    "    # first set of weights from input to hidden layer 1\n",
    "    model['W1'] = np.random.randn(input_dim, hidden_nodes)/np.sqrt(input_dim)\n",
    "    # set of biases\n",
    "    model['b1'] = np.zeros((1, hidden_nodes))\n",
    "    \n",
    "    # second set of weights from hidden layer 1 to layer 2\n",
    "    model['W2'] = np.random.randn(hidden_nodes, output_dim)/np.sqrt(hidden_nodes)\n",
    "    # set of biases for second hidden layer\n",
    "    model['b2'] = np.zeros((1, hidden_nodes))\n",
    "    \n",
    "    # third set of weights from hidder layer 2 to output\n",
    "    model['W3'] = np.random.randn(hidden_nodes, output_dim)/np.sqrt(hidden_nodes)\n",
    "    # set of biases for output layer\n",
    "    model['b3'] = np.zeros((1, output_dim))\n",
    "    \n",
    "    return model\n",
    "\n",
    "# defines the forward pass given a model and data\n",
    "def feed_forward(model, x):\n",
    "    # get weights and biases\n",
    "    W1, b1, W2, b2, W3, b3 = model['W1'], model['b1'], model['W2'], model['b2'], model['W3'], model['b3']\n",
    "    # first layer\n",
    "    z1 = x.dot(W1) + b1\n",
    "    \n",
    "    # activation function\n",
    "    a1 = model['activation_function'](z1)\n",
    "    \n",
    "    # second layer\n",
    "    z2 = a1.dot(W2) + b2\n",
    "    \n",
    "    # activation function\n",
    "    a2 = model['activation_function'](z2)\n",
    "    \n",
    "    # third layer\n",
    "    z3 = a2.dot(W3) + b3\n",
    "    \n",
    "    # no activation function as this is simply a linear layer!!\n",
    "    out = z3\n",
    "    return z1, a1, z2, a2, z3, out\n",
    "\n",
    "# define the regression loss\n",
    "def calculate_loss(model,X,y,reg_lambda):\n",
    "    num_examples = X.shape[0]\n",
    "    W1, b1, W2, b2, W3, b3 = model['W1'], model['b1'], model['W2'], model['b2'], model['W3'], model['b3']\n",
    "    \n",
    "    # what are the current predictions\n",
    "    z1, a1, z2, a2, z3, out = feed_forward(model, X)\n",
    "    \n",
    "    # calculate L2 loss\n",
    "    loss = 0.5*np.sum((out-y)**2)\n",
    "    \n",
    "    # add regulatization term to loss\n",
    "    loss += reg_lambda/2*(np.sum(np.square(W1))+ np.sum(np.square(W2)) + np.sum(np.square(W3)))\n",
    "    \n",
    "    # return per-item loss\n",
    "    return 1./num_examples * loss\n",
    "\n",
    "# back-propagation for the two-layer network\n",
    "def backprop(X,y,model,z1,a1,z2,a2, z3, output,reg_lambda):\n",
    "    \n",
    "    # derivative of loss function\n",
    "    delta4 = (output-y)/X.shape[0]\n",
    "    \n",
    "    # multiply this by activation outputs of second hidden layer\n",
    "    dW3 = (a2.T).dot(delta4)\n",
    "    # and over all neurons\n",
    "    db3 = np.sum(delta4, axis=0, keepdims=True) #different because it is not being multiplied by all the weights, only 1\n",
    "    \n",
    "    # derivative of activation function of first hidden layer\n",
    "    delta3 = delta4.dot(model['W3'].T) * model['activation_function_derivative'](a2)\n",
    "    \n",
    "    # multiply by weight data of previous layer\n",
    "    dW2 = (a1.T).dot(delta3)\n",
    "    # and over all neurons\n",
    "    db2 = np.sum(delta3, axis=0)\n",
    "    \n",
    "    # derivative of activation function of secpnd hidden layer\n",
    "    delta2 = delta3.dot(model['W2'].T) * model['activation_function_derivative'](a1)\n",
    "    \n",
    "    # multiply by input data\n",
    "    dW1 = (X.T).dot(delta2)\n",
    "    # and sum over all neurons\n",
    "    db1 = np.sum(delta2, axis=0)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # multiply this by activation outputs of hidden layer\n",
    "#     dW2 = (a1.T).dot(delta3)\n",
    "    \n",
    "#     # and over all neurons\n",
    "#     db2 = np.sum(delta3, axis =0, keepdims=True) #different because it is not being multiplied by all the weights, only 1\n",
    "    \n",
    "#     # derivative of activation function\n",
    "#     delta2 = delta3.dot(model['W2'].T)* model['activation_function_derivative'](a1) # call the appropriate activation function derivative\n",
    "    \n",
    "    \n",
    "    \n",
    "#     # multiply by input data\n",
    "#     dW1 = (X.T).dot(delta2)\n",
    "#     # and sum over all neurons\n",
    "#     db1 = np.sum(delta2, axis=0)\n",
    "    \n",
    "    # add regularization terms on the three weights\n",
    "    dW3 += reg_lambda * model['W3']\n",
    "    dW2 += reg_lambda * model['W2']\n",
    "    dW1 += reg_lambda * model['W1']\n",
    "    \n",
    "    return dW1, dW2, dW3, db1, db2, db3\n",
    "\n",
    "# simple training loop\n",
    "def train(model, X, y, num_passes=100000, reg_lambda = 0.1, learning_rate = 0.001):\n",
    "    # whether to do stochastic gradient descent\n",
    "    sgd = True\n",
    "    \n",
    "    # variable that checks whether we break iteration\n",
    "    done = False\n",
    "    \n",
    "    # keeping track of losses\n",
    "    previous_loss = float('inf')\n",
    "    losses = []\n",
    "\n",
    "    # iteration counter\n",
    "    i = 0\n",
    "    while done == False:\n",
    "        if sgd:\n",
    "            # choose a random set of points\n",
    "            randinds = np.random.choice(np.arange(len(y)),30,False) #bad programming because we dont know the number of  data points needed , 30 is random\n",
    "            # get predictions\n",
    "            z1,a1,z2,a2,z3,output = feed_forward(model, X[randinds,:])\n",
    "            # feed this into backprop\n",
    "            dW1, dW2, dW3, db1, db2, db3 = backprop(X[randinds,:],y[randinds],model,z1,a1,z2,a2,z3,output,reg_lambda)\n",
    "        else:\n",
    "            # get predictions\n",
    "            z1,a1,z2,output = feed_forward(model, X)\n",
    "            # feed this into backprop\n",
    "            dW1, dW2, db1, db2 = backprop(X,y,model,z1,a1,z2,a2,z3,output,reg_lambda)\n",
    "            \n",
    "        # given the results of backprop, update both weights and biases\n",
    "        model['W1'] -= learning_rate * dW1\n",
    "        model['b1'] -= learning_rate * db1\n",
    "        model['W2'] -= learning_rate * dW2\n",
    "        model['b2'] -= learning_rate * db2\n",
    "        model['W3'] -= learning_rate * dW3\n",
    "        model['b3'] -= learning_rate * db3\n",
    "        \n",
    "        # do some book-keeping every once in a while\n",
    "        if i % 1000 == 0:\n",
    "            loss = calculate_loss(model, X, y, reg_lambda)\n",
    "            losses.append(loss)\n",
    "            print(\"Loss after iteration {}: {}\".format(i, loss))\n",
    "            # very crude method to break optimization\n",
    "            if np.abs((previous_loss-loss)/previous_loss) < 0.001:\n",
    "                done = True\n",
    "            previous_loss = loss\n",
    "        i += 1\n",
    "        if i>=num_passes:\n",
    "            done = True\n",
    "    return model, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e886fd20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_function': <function relu at 0x0000021867955E10>, 'activation_function_derivative': <function relu_derivative at 0x000002181874F370>, 'W1': array([[ 0.39040071,  0.47263102,  0.21027132,  2.06978409,  0.18969113,\n",
      "         0.78562218, -0.11954799,  0.36843934, -0.15633259,  0.59521464],\n",
      "       [-0.73791726,  0.53136984, -0.13753878,  0.76988615, -0.95843127,\n",
      "        -0.03902126,  0.7910486 , -0.39877785, -0.67435882, -0.80161998]]), 'b1': array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), 'W2': array([[ 0.12113415],\n",
      "       [ 0.62189023],\n",
      "       [-0.1221685 ],\n",
      "       [ 0.33186811],\n",
      "       [ 0.49213712],\n",
      "       [ 0.09383218],\n",
      "       [-0.65176524],\n",
      "       [ 0.13872847],\n",
      "       [ 0.07414862],\n",
      "       [-0.42954013]]), 'b2': array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), 'W3': array([[ 0.07916346],\n",
      "       [ 0.7159567 ],\n",
      "       [-0.51845622],\n",
      "       [-0.09280125],\n",
      "       [-0.11845741],\n",
      "       [-0.09217957],\n",
      "       [-0.22712933],\n",
      "       [-0.09209621],\n",
      "       [ 0.20456262],\n",
      "       [-0.46770926]]), 'b3': array([[0.]])}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "non-broadcastable output operand with shape (10,1) doesn't match the broadcast shape (10,10)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [28], line 30\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(model)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# train it\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m model, losses \u001b[38;5;241m=\u001b[39m train(model,X, y, reg_lambda\u001b[38;5;241m=\u001b[39mreg_lambda, learning_rate\u001b[38;5;241m=\u001b[39mlearning_rate)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# determine predictions of the trained model\u001b[39;00m\n\u001b[0;32m     33\u001b[0m output \u001b[38;5;241m=\u001b[39m feed_forward(model, X)\n",
      "Cell \u001b[1;32mIn [27], line 159\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, X, y, num_passes, reg_lambda, learning_rate)\u001b[0m\n\u001b[0;32m    157\u001b[0m model[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW1\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m learning_rate \u001b[38;5;241m*\u001b[39m dW1\n\u001b[0;32m    158\u001b[0m model[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb1\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m learning_rate \u001b[38;5;241m*\u001b[39m db1\n\u001b[1;32m--> 159\u001b[0m model[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW2\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m learning_rate \u001b[38;5;241m*\u001b[39m dW2\n\u001b[0;32m    160\u001b[0m model[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb2\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m learning_rate \u001b[38;5;241m*\u001b[39m db2\n\u001b[0;32m    161\u001b[0m model[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW3\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m learning_rate \u001b[38;5;241m*\u001b[39m dW3\n",
      "\u001b[1;31mValueError\u001b[0m: non-broadcastable output operand with shape (10,1) doesn't match the broadcast shape (10,10)"
     ]
    }
   ],
   "source": [
    "numDataOne = 15\n",
    "numData = numDataOne*numDataOne\n",
    "# create data for regression\n",
    "xs=np.linspace(-8,8,numDataOne)\n",
    "ys=np.linspace(-8,8,numDataOne)\n",
    "counter=0\n",
    "X=np.zeros((numData,2))\n",
    "y=np.zeros((numData,1))\n",
    "for r in np.arange(0,numDataOne):\n",
    "    for c in np.arange(0,numDataOne):\n",
    "        X[counter,:]=[xs[r],ys[c]]\n",
    "        y[counter]=xs[r]**2+ys[c]**2+1\n",
    "        counter=counter+1\n",
    "\n",
    "# training set size\n",
    "num_examples = len(X) \n",
    "# input layer dimensionality\n",
    "nn_input_dim = 2 \n",
    "# output layer dimensionality\n",
    "nn_output_dim = 1  \n",
    "# learning rate for gradient descent\n",
    "learning_rate = 0.001\n",
    "# regularization strength\n",
    "reg_lambda = 0.01 \n",
    "\n",
    "# create the model\n",
    "model = create_model(X,10,1) # try changing number of neurons from 10 to 4 to 2 to 3 to 100 (to see how the final 3d image is altered)\n",
    "print(model)\n",
    "# train it\n",
    "model, losses = train(model,X, y, reg_lambda=reg_lambda, learning_rate=learning_rate)\n",
    "\n",
    "# determine predictions of the trained model\n",
    "output = feed_forward(model, X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
